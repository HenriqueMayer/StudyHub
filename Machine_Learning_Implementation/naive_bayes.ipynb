{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Naive Bayes is a probabilistic classifier that applies Bayes' Theorem with the assumption of strong (naive) independence between features.\n",
    "\n",
    "## Bayes' Theorem\n",
    "The theorem is expressed as:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "### In the context of classification:\n",
    "For a feature vector \\( X = (x_1, x_2, \\ldots, x_n) \\), the probability of class \\( y \\) given \\( X \\) is:\n",
    "\n",
    "$$\n",
    "P(y|X) = \\frac{P(X|y) \\cdot P(y)}{P(X)}\n",
    "$$\n",
    "\n",
    "## Independence Assumption\n",
    "Assuming the features are mutually independent, the equation simplifies to:\n",
    "\n",
    "$$\n",
    "P(y|X) = \\frac{P(x_1|y) \\cdot P(x_2|y) \\cdot \\ldots \\cdot P(x_n|y) \\cdot P(y)}{P(X)}\n",
    "$$\n",
    "\n",
    "## Decision Rule\n",
    "The predicted class \\( y \\) is determined by maximizing the posterior probability:\n",
    "\n",
    "$$\n",
    "y = \\arg\\max_y \\left( \\log(P(x_1|y)) + \\log(P(x_2|y)) + \\ldots + \\log(P(x_n|y)) + \\log(P(y)) \\right)\n",
    "$$\n",
    "\n",
    "## Key Components:\n",
    "- **Prior Probability (\\( P(y) \\))**: The probability of each class based on frequency.\n",
    "- **Class-Conditional Probability (\\( P(x_i|y) \\))**: The probability of a feature given a class, often modeled with a Gaussian distribution.\n",
    "\n",
    "## Training Process\n",
    "1. Calculate the mean and variance for each feature within each class (for Gaussian models).\n",
    "2. Compute the prior probability for each class based on its frequency in the dataset.\n",
    "\n",
    "Naive Bayes is particularly useful for high-dimensional data and is commonly used in text classification and spam filtering. Its simplicity and efficiency make it a powerful baseline classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # Calculate mean, var, and prior for each class\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64) \n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for idxs, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "            self._mean[idxs, :] = X_c.mean(axis=0)\n",
    "            self._var[idxs, :] = X_c.var(axis=0)\n",
    "            self._priors[idxs] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    # Helper Fucntion\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # Calculate posterior probability for each class\n",
    "        for idxs, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idxs])\n",
    "            posterior = np.sum(np.log(self._pdf(idxs, x)))\n",
    "            posterior = posterior + prior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # Return class with the heighest posterior\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "    \n",
    "    def _pdf(self, class_idx, x): # PT: fdp (função densidade probabilidade)\n",
    "        mean  = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(-((x-mean)**2) / (2*var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        \n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    acc = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return acc\n",
    "\n",
    "\n",
    "X, y = datasets.make_classification(\n",
    "    n_samples=1000, n_features=10, n_classes=2, random_state=1234\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1234\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93\n"
     ]
    }
   ],
   "source": [
    "clf = NaiveBayes()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "acc = accuracy(y_test, predictions)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
